{"cells":[{"cell_type":"code","source":["from pyspark.sql import DataFrame\n","from pyspark.sql.types import IntegerType, StringType, BooleanType, FloatType, StructType, StructField\n","from pyspark.sql.functions import col, to_date, dayofmonth\n","from pyspark.ml.feature import StringIndexer\n","from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml import Pipeline\n","from pyspark.ml.regression import LinearRegression\n","from pyspark.ml.evaluation import RegressionEvaluator"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"54e70036-7287-4400-86c9-f6287789f47e","inputWidgets":{},"title":""},"id":"kAUcO_lZf5Wx"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["## Dataset\n","\n","We'll consider the San Francisco Fire Department Calls dataset.\n","\n","```\n","/databricks-datasets/learning-spark-v2/sf-fire/sf-fire-calls.csv\n","```\n","\n","## Part 1\n","\n","Write a function named `prepare_dataframe()` that\n","\n","- reads the csv file\n","- excludes the records with `CallType` that occurs less than `200` times in the entire dataset (it is fine to hardcode the values manually)\n","- computes a new column `CallDayOfWeek` that computes the day of the week (1 Sunday, 7 Saturday) based on the column `CallDate`\n","- computes a new column `DeltaPriority` that computes the difference between `FinalPriority` and `OriginalPriority` (eg `DeltaPriority=2` when `FinalPriority=4` and `OriginalPriority=2`)\n","- keeps only the following columns `CallNumber`, `CallType`, `Priority`, `CallDayOfWeek`, `DeltaPriority`, `Delay`\n","- remove rows where any record is null (you can use `dropna`)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4ae1b809-0540-484d-ab74-1fcaf649c0ba","inputWidgets":{},"title":""},"id":"EO2y4QVpf5Wy"}},{"cell_type":"code","source":["def prepare_dataframe() -> DataFrame:\n","    # TASK 1\n","    fire_schema = StructType([\n","    StructField('CallNumber', IntegerType(), True),\n","    StructField('UnitID', StringType(), True),\n","    StructField('IncidentNumber', IntegerType(), True),\n","    StructField('CallType', StringType(), True),\n","    StructField('CallDate', StringType(), True),\n","    StructField('WatchDate', StringType(), True),\n","    StructField('CallFinalDisposition', StringType(), True),\n","    StructField('AvailableDtTm', StringType(), True),\n","    StructField('Address', StringType(), True),\n","    StructField('City', StringType(), True),\n","    StructField('Zipcode', IntegerType(), True),\n","    StructField('Battalion', StringType(), True),\n","    StructField('StationArea', StringType(), True),\n","    StructField('Box', StringType(), True),\n","    StructField('OriginalPriority', IntegerType(), True),\n","    StructField('Priority', IntegerType(), True),\n","    StructField('FinalPriority', IntegerType(), True),\n","    StructField('ALSUnit', BooleanType(), True),\n","    StructField('CallTypeGroup', StringType(), True),\n","    StructField('NumAlarms', IntegerType(), True),\n","    StructField('UnitType', StringType(), True),\n","    StructField('UnitSequenceInCallDispatch', IntegerType(), True),\n","    StructField('FirePreventionDistrict', StringType(), True),\n","    StructField('SupervisorDistrict', StringType(), True),\n","    StructField('Neighborhood', StringType(), True),\n","    StructField('Location', StringType(), True),\n","    StructField('RowID', StringType(), True),\n","    StructField('Delay', FloatType(), True)\n","    ])\n","    df = spark.read.csv(\"/databricks-datasets/learning-spark-v2/sf-fire/sf-fire-calls.csv\",header=True, schema=fire_schema)\n","    \n","    # TASK 2\n","    # display(df.groupby(df.CallType)\n","    #            .count()\n","    #            .where(col(\"count\")<200)\n","    #            .select(df.CallType)\n","    #            .collect())\n","    df=df.where(df.CallType != \"Administrative\")\n","    df=df.where(df.CallType != \"Train / Rail Fire\")\n","    df=df.where(df.CallType != \"Lightning Strike (Investigation)\")\n","    \n","    # TASK 3\n","    df=(df.withColumn(\"Call_to_Date\",to_date(col(\"CallDate\"), \"dd/MM/yyyy\"))\n","        .withColumn(\"CallDayOfWeek\", dayofmonth(col(\"Call_to_Date\")))\n","       )\n","    # TASK 4\n","    df=df. withColumn(\"DeltaPriority\",df.FinalPriority - df.OriginalPriority)\n","    \n","    # TASK 5\n","    df=df.select(\"CallNumber\",\"CallType\", \"Priority\", \"CallDayOfWeek\", \"DeltaPriority\", \"Delay\")\n","    \n","    # TASK 6\n","    df = df.dropna()\n","    \n","    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e0014084-11c2-4806-aef3-144d24b36155","inputWidgets":{},"title":""},"id":"r9_HYB6Nf5Wz"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["## Part 2\n","\n","Write a function named `define_pipeline()` that\n","\n","- returns a `Pipeline` object\n","- the object is initialized with stages that predict the `Delay` via a linear regression based on the following features: `CallType`, `Priority`, `CallDayOfWeek`, `DeltaPriority`. Two of these features can be considered as categorical, which are the 2?"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"501fec14-986c-4715-a91b-f09a9037b3e7","inputWidgets":{},"title":""},"id":"_GVkRUdcf5W3"}},{"cell_type":"code","source":["def define_pipeline() -> Pipeline:\n","    indexer = StringIndexer(\n","        inputCols = [\"CallType\",\"CallDayOfWeek\"],\n","        outputCols=[\"CallType_indexed\", \"CallDayOfWeek_indexed\"]\n","    )\n","    \n","    assembler = VectorAssembler(\n","        inputCols = [\"CallType_indexed\",\"Priority\",\"CallDayOfWeek_indexed\",\"DeltaPriority\"],\n","        outputCol= \"features\"\n","    )\n","    \n","    regressor= LinearRegression(\n","        labelCol=\"Delay\",\n","        featuresCol=\"features\"\n","    )\n","    \n","    pipeline = Pipeline(\n","        stages=[indexer, assembler, regressor]\n","    )\n","    return pipeline"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c12ea059-cf27-4477-8c67-0539f19ee7b8","inputWidgets":{},"title":""},"id":"Qe8tTKd0f5W3"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["## Part 3\n","\n","Write a function named `fit_model(df, pipeline)` that\n","- takes as input\n","  - `df`, a `DataFrame` object that corresponds to the one that is returned by `prepare_dataframe()`\n","  - `pipeline`, a `Pipeline` object that corresonds to the one that is returned by `define_pipeline()`\n","- splits `df` in _train_ and _test_ dataset (80% and 20% respectively using a seed of `42`)\n","- fits the `pipeline` on the _train_ dataset and runs prediction on the _test_ dataset\n","- returns the `DataFrame` of the predictions made of only 2 columns: `DelayLabel`, `DelayPrediction`"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3c7c7915-21fb-4845-855c-fbf4b40a324a","inputWidgets":{},"title":""},"id":"yIl4vzw9f5W5"}},{"cell_type":"code","source":["def fit_model(df: DataFrame, pipeline: Pipeline) -> DataFrame:\n","    trainDF,testDF = df.randomSplit([0.8, 0.2], seed=42)\n","    \n","    model = pipeline.fit(trainDF)\n","    delay_predictions = model.transform(testDF)\n","    \n","    predictions_df = delay_predictions.select(\"Delay\",\"prediction\")\n","    \n","    return predictions_df\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"be22dda9-b4e4-4aaf-8bba-926a3d671b0c","inputWidgets":{},"title":""},"id":"Z6yAnzJwf5W6"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["## Part 4\n","Write a function `evaluate_r2(predictions_df)` that\n","- takes as input a `DataFrame` object that corresponds to the one that is returned by `fit_model()`, ie made of 2 columns `DelayLabel` and `DelayPrediction`\n","- computes the R squared metric and returns the R2 value"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"866a55f8-48be-4086-94a8-df525dc84e34","inputWidgets":{},"title":""},"id":"KvF0vlfNf5W7"}},{"cell_type":"code","source":["def evaluate_r2(predictions_df: DataFrame) -> float:\n","    evaluator = RegressionEvaluator(\n","        predictionCol=\"prediction\",\n","        labelCol=\"Delay\",\n","        metricName=\"r2\"\n","    )\n","    \n","    r2 = evaluator.evaluate(predictions_df)\n","    \n","    return r2"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"574664f4-b0f7-40b1-8a53-b87fc3ea2ef3","inputWidgets":{},"title":""},"id":"vn3MynTff5W7"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["df = prepare_dataframe()\n","\n","pipeline = define_pipeline()\n","\n","predictions = fit_model(df = df, pipeline = pipeline)\n","\n","r2 = evaluate_r2(predictions)\n","display (\"r2: \",r2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"03f46a66-c76c-405e-b680-db56999f1e7e","inputWidgets":{},"title":""},"id":"9xVs7I2mf5W8","outputId":"52a266ed-f5df-4b77-9239-3c3a3bfd4a24"},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"'r2: '0.004543142194121441","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"data":{"text/plain":["'r2: '0.004543142194121441"]}}],"execution_count":null}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"221108_exam","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1465508999835320},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}